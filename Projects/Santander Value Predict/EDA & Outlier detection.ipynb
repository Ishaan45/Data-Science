{
  "cells": [
    {
      "metadata": {
        "_uuid": "efb6dbeb49edd5e10348a241eff2b95e31b1ee94"
      },
      "cell_type": "markdown",
      "source": "The trend of anonymized data for online competitions is increasing day by day as companies want their data to be secure and thus maintaining the privacy of their customers. Santander has released an anonymized dataset for predicting the value of transactions for each potential customer.\n\nSo in this notebook I will be focusing on gathering insights from the unknown data."
    },
    {
      "metadata": {
        "_uuid": "21d124caaedd9f7ea03f368f195008f84fde9dd2"
      },
      "cell_type": "markdown",
      "source": "# Importing modules and getting a glimpse of the data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "355d57c528cfa93ba406856fa519657a8c50254b",
        "scrolled": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import TruncatedSVD\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport os\nprint(os.listdir(\"../input\"))\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nfrom sklearn.ensemble import IsolationForest\nimport matplotlib.pyplot as plt",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8bc51795df45a45dac62b732864dccd164c6ab84",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "cb40cd1f93cf625f905562b7513b91873626a104",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_data.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cfad1a7f24189f8c161c578df48a6e26b58b631b",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "test_data.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9639a7d9192d5d7e3c1702607d1129ef42c6fe7d",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(\"The shape of the training set is:\",train_data.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1da68fae113ab2779324abb999cc223d84430a61",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(\"The shape of the test set is:\", test_data.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d04d4558bee437764ff4fc4f185591e1125c18f5"
      },
      "cell_type": "markdown",
      "source": "- It is quiet interesting to see that the number of features in the train dataset is greater than the number of data points i.e. **the curse of dimensionality**.\n- The test set is 10 times bigger than the train set in shape.\n- Thus, feature extraction is very important and will substantially improve the score of the model."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c68e6b83a791b9fd98a85a152fa7aadacb67c17",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "feature_cols = [c for c in train_data.columns if c not in [\"ID\", \"target\"]]\nflat_values = train_data[feature_cols].values.flatten()\n\nlabels = 'Zero_values','Non-Zero_values'\nvalues = [sum(flat_values==0), sum(flat_values!=0)]\ncolors = ['rgba(55, 12, 93, .7)','rgba(125, 42, 123, .1)']\n\nPlot = go.Pie(labels=labels, values=values,marker=dict(colors=colors,line=dict(color='#fff', width= 3)))\nlayout = go.Layout(title='Value distribution', height=380)\nfig = go.Figure(data=[Plot], layout=layout)\niplot(fig)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fb84f801e88e744fc98809f0beb90a4e50c6fee6",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_data.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "47032a3f245835ed1f523ffa4333239e1d5d2a03"
      },
      "cell_type": "markdown",
      "source": "The memory usage of the data is approx 170MB and the datatypes for features are distributed as:\n- **float64** - 1845\n-   **int64** - 3147\n-  **object** - 1"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0289aefd8f8f1c418946e2a74ba914673f0d5441",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "test_data.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "50a78693ba4274cf36382b09bcb696dccbfc21c8"
      },
      "cell_type": "markdown",
      "source": "The memory usage for test data is 1.8GB and the datatypes for features are:\n\n- **float64** - 4991\n-  **object** - 1"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "035b180a7b96bc9f11847cd659e8fa1fc6956b9a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_data.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "632b1f284275536599d9670c0b1a9f6aa414ebf9"
      },
      "cell_type": "code",
      "source": "def missing_data(data): #calculates missing values in each column\n    total = data.isnull().sum().reset_index()\n    total.columns  = ['Feature_Name','Missing_value']\n    total_val = total[total['Missing_value']>0]\n    total_val = total.sort_values(by ='Missing_value')\n    return total_val",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "58caf7bff8d74594b7deca152c2c03340943d207",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "missing_data(train_data).head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "97ade4e1b7a1d3b926da17f44a7ed48e167ced2d",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "missing_data(test_data).head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6a70ff8e7aa426a7989567a64d09a0b94d1f0922"
      },
      "cell_type": "markdown",
      "source": "There are no missing values in the train and test dataset. This is reasonably good as it is nearly impossible to fill missing data with certain values. "
    },
    {
      "metadata": {
        "_uuid": "3c408667768d6ca5c0fca69c1665b9c528fa6b00"
      },
      "cell_type": "markdown",
      "source": "As the data is sparse, It is required for us to drop the features having constant value throughout the dataset as they will just increase the dimensionality of the dataset hampering the prediction of the target value in the test set. \n\nA feature is constant if the number of unique elements in it is equal to 1 i.e. nunique =1.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "23c76feef5d7d147daec24980420607b51c14ff2",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#train_data = train_data.loc[:,train_data.apply(pd.Series.nunique) != 1]\n#train_data.shape\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "594ab0a11916f73b6fc94ea5a81959192b4c9be1"
      },
      "cell_type": "markdown",
      "source": "# Feature selection using Truncated SVD"
    },
    {
      "metadata": {
        "_uuid": "55dc4f96215a37473c15dc667513c058e73c0fd6"
      },
      "cell_type": "markdown",
      "source": "To avoid the curse of dimensionality, apart from PCA we can apply linear dimensionality reduction by the means of truncated singular value decomposition.This estimator does not center the data.\n\nIn practice, TruncatedSVD is very useful for highly sparse datasets which cannot be centered without making the memory usage explode. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0d41e633f0fa89b83d60f908b64c2a76b4d69e6a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "X = train_data.drop(['ID','target'],axis=1)\ny_train = train_data[\"target\"]\nX_test = test_data.drop('ID', axis = 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6b56a20cac166bf84af1dbe62b621efee16dfdeb",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#svd = TruncatedSVD(n_components=1300, random_state=0)\n#SVD_result =svd.fit(X_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c10a4f5441f0073e2be97570956b0ba5a805ce2e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#cumm_perc = np.sum(SVD_result.explained_variance_ratio_)\n#print(\"Cumulative explained variation for 1300 components:\"+\"{:.2%}\".format(cumm_perc))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "72145d6cb36decefda36e23219148d8a2029ea8b"
      },
      "cell_type": "markdown",
      "source": "From the above results it is evident that first 1300 principal components results for 99.2% of the variance in the train dataset.  "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6d7a4f928b4c8cb52c1b17ef4c15f0087e7ea5ba",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#X_mod = SVD_result.transform(X_train)\n#X_mod_test = SVD_result.fit_transform(X_test)\n\n#y = np.log1p(y_train.values)\n\n#X_train =scaler.fit_transform(X_train)\n#X_test = scaler.fit_transform(X_test)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bf31ed215749a2f6fad2194836c69052221405bb",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Outlier detection using Isolation Forest "
    },
    {
      "metadata": {
        "_uuid": "132fbcd140dd10048b6af1470eb962b395ad3aed"
      },
      "cell_type": "markdown",
      "source": "Outlier detection is one of the most important aspects of regression analysis. If not removed it can hamper the performance of the model which we will fit to the data for continuous value prediction. So I have used a method which is highly suitable for high dimensional datasets i.e. Isolation forest algorithm, an ensemble method which returns anomaly scores of each sample in the dataset.\n\nThe IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n\n+1 indicates that the sample is an inlier whereas -1 indicates that the sample is an outlier."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3f63ae2babb5547be5c8553e1222c3bacfe7bda9",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "clf = IsolationForest(max_samples=100, random_state=0)\nclf.fit(X)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "433dab773e1dbfbcd73d212d88ebd946c5879e0a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "y_pred = clf.predict(X)\ny_pred_df = pd.DataFrame(data=y_pred,columns = ['Values'])\ny_pred_df['Values'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3a817808a3421bd9513f17574534132598a04000",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "anomaly_score = clf.decision_function(X)\nanomaly_score",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7627cbb9783479eaced98746efc37af4689036c5",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "y_test_pred = clf.predict(X_test)\ny_test_pred_df = pd.DataFrame(data=y_test_pred,columns = ['Out_Values'])\ny_test_pred_df['Out_Values'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "0ef09c4f059d40dc08bd92b3edc0825e5adc80fd"
      },
      "cell_type": "code",
      "source": "anomaly_score = clf.decision_function(X_test)\nanomaly_score",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7296abbafba743bdd0995f2542a64fa69c1af400",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#sub = pd.read_csv('../input/sample_submission.csv')\n#sub[\"target\"] = \n#print(sub.head())\n#sub.to_csv('sub_xgb.csv', index=False)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "323d03344ced857ea5c7019a7fbbe243b10985c1"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "5af5047be74b4677b46e3b95ea98d269572e094e"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "8dbb3c8820920b3fbd489883a22ae80950b0b971"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}